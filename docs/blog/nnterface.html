<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="/static/css/main.css">
    <link rel="stylesheet" href="/static/css/breadcrumbs.css">
    <link rel="stylesheet" href="/static/css/cord.css"> 
    <title>NNterface</title>

    <script>
        const savedMode = localStorage.getItem('darkMode');
        if (savedMode === 'enabled') {
            document.documentElement.classList.add('dark-mode');
        }
    </script>
    
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:ital,opsz,wght@0,14..32,100..900;1,14..32,100..900&display=swap" rel="stylesheet">
</head>

<body class="container">
    <div id="pull-cord">
        <div id="cord-line"></div>
        <div id="cord-handle"></div>
    </div>

    <div class="inner">
        <ul class="breadcrumb">
            <li><a href="/">root</a></li>
            <li><a href="/blog.html">blog</a></li>
            <li>NNterface</li>
        </ul>
        <h1>NNterface</h1>
<style>
    html {
        background-color: #FFFCF0 !important;

        line-height: 1.5rem;
    }
    html.dark-mode {
        background-color: #100F0F !important;
    }

    h1 {
        font-size: 2em !important;
    }

    h2 {
        font-size: 1.5em !important;
    }

    body {
        font-size: 1.25em !important;
    }

    img {
        border-style: solid;
        border-width: 2px;
        border-color: #282726;
    }

    .example-box {
        transition: transform 0.2s ease-in-out;
    }

    .example-box:hover {
        transform: scale(1.05);
    }
</style>

<nav class="toc">
    <ul>
        <li><a href="#motivation">Motivation</a></li>
        <li><a href="#example-logit-lens">Example: Logit Lens</a></li>
        <li><a href="#example-steering">Example: Steering</a></li>
        <li><a href="#example-patching">Example: Patching</a></li>
        <li><a href="#looking-forward">Looking Forward</a></li>
        <li><a href="#acknowledgements">Acknowledgements</a></li>
    </ul>
</nav>

<div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 20px; margin: 20px 0;">
    <div class="example-box" style="padding: 15px; border: 1px solid #282726; border-radius: 5px;">
        <h3>Item 1</h3>
        <p>First grid item content</p>
    </div>
    <div class="example-box" style="padding: 15px; border: 1px solid #282726; border-radius: 5px;">
        <h3>Item 2</h3>
        <p>Second grid item content</p>
    </div>
    <div class="example-box" style="padding: 15px; border: 1px solid #282726; border-radius: 5px;">
        <h3>Item 3</h3>
        <p>Third grid item content</p>
    </div>
</div>

<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

<p>This post is a writeup for some early ideas of what a whitebox AI interface might look like.</p>
<h2>Motivation</h2>
<p>How do we make interpretability research accessible? </p>
<p>I built an interface for white box interaction with language models. It provides a simple, visual language for expressing interventions. I’ll illustrate its usage by implementing three common interpretability techniques.</p>
<h2>Example: Logit Lens</h2>
<p>Logit lens is an early exit technique which directly decodes hidden states into vocabulary space using the model’s unembedding matrix. </p>
<p>We investigate how the model completes the sentence “The Eiffel Tower is located in the city of”. To do so, we’ll build a small program to track the probability of the correct token, <code>_Paris</code>, at each layer.</p>
<p>Every program in the interface starts with a run context. The context represents a single forward pass of a model; in this example, we’ve loaded GPT2 for study. We’d like to view the probability for the token <code>_Paris</code> at each layer of GPT2, so we drag a module node from the tree into the canvas. To capture activations from a module, simply drag from its handle. Connecting to another valid handle creates a directed edge, denoting a get operation. </p>
<p>Transformers are composed of a series of identical layers. We wrap the module node with a loop context to get activations at each layer. The context accepts some start/end indices and returns a variable which we enter as the index in our module node. GPT2 has twelve layers, so we loop accordingly.</p>
<p>Function nodes enable arbitrary code injection, allowing any PyTorch program to be represented in the interface. Within the code editor, we enter some arguments and write two short lines of code that return the probability over <code>_Paris</code>.</p>
<p>[[side by side of editor and function in python, showing how they are the same]]</p>
<p>We append the output of the function to a list. Finally, we add a graph node at the end to visualize the outputs. When we execute the program, we can open up a graphing panel to the right where we can visualize the outputs of the intervention. </p>
<p><img src="../static/images/nnterface/lens.gif" alt="Logit Lens" width="500" loading="lazy"></p>
<p>Figure 1: This graph displays the probability over the token <code>_Paris</code> at each layer of GPT2.</p>
<h2>Example: Steering</h2>
<p>Turner et al. 2023 demonstrate a simple method where combining forward passes can be used to steer completions. We replicate an example from their post to steer GPT2 to talk about weddings in unrelated contexts. </p>
<p><img src="../static/images/nnterface/steer.gif" alt="Logit Lens" width="500" loading="lazy"></p>
<p>Here, we use two run contexts for two forward passes: one to collect activations from a wedding prompt and one to steer. We get the wedding activation at some middle layer and add it to the activation of a different prompt. An edge to a module handle denotes a set operation, replacing the activations. </p>
<p>This method of activation steering is a little finicky; we can open the toolbar on the right to check whether the prompts are of compatible length. </p>
<p>We generate multiple tokens by opening the chat tab to the right. Text we write here is passed as input to a run context that uses the chat node. </p>
<p><img src="../static/images/nnterface/chat.gif" alt="Logit Lens" width="500" loading="lazy"></p>
<p>In fact, we can literally chat with our interventions; loading an instruct or chat tuned model opens a conversational interface.</p>
<h2>Example: Patching</h2>
<p>Activation patching highlights the causal importance of certain points during a model’s computation. This technique is more involved, so I’ll mostly highlight features rather than discussing implementation details.</p>
<p>[[picture of intervention with different passes labeled with numbers]]</p>
<p>We run the model three times. (1) We compute a baseline prediction from a “clean” prompt. (2) Then we run a “corrupted” prompt in which the subjects are swapped. We cache the layer activations and logits over the “corrupted” completion. </p>
<p>(3) Finally, we run the model on a corrupted input but restore activations at different token/layer positions. Here, we use the batch context to run multiple inputs in a single forward pass. For each batched run, the effect is: </p>
<p>$$\text{effect} = \frac{\text{restored} - \text{corrupted}}{\text{clean} - \text{corrupted}}$$</p>
<p>We visualize the intervention as before, this time with a heatmap.</p>
<p>[[gif of running the intervention, selecting heatmap]]</p>
<p>This intervention is more complex, but it follows from the primitives discussed above. Arguably the coolest feature of NNterface is the ability to export our interventions to NNsight code in Python!</p>
<p><img src="../static/images/nnterface/export.gif" alt="Logit Lens" width="500" loading="lazy"></p>
<h2>Looking forward</h2>
<p>I had a lot of fun learning Svelte and writing the visual compiler for this project, but I don’t plan on pursuing it further. At the point a user is learning my visual language, they might as well learn NNsight. It's a beautiful library with a similar goal of separability between experimentation and engineering in Python. </p>
<p>However, this project is a great way to communicate interpretability techniques. For example, an inexperienced user could play around with activation patching, testing different metrics or saving outputs from different modules. The hands-on experience without a coding prior makes it easy to learn about interventions.</p>
<p>There are a couple immediate extensions from my final product. </p>
<ul>
  <li>Activation patching is a lot to set up each time. A component tab would allow users to merge a set of connected nodes into a component. For example, an activation patching component might accept a corrupted prompt and return a list of layer outputs and final logits.</li>
  <li>Currently, NNterface is only available as a local installation. Scaling interventions is hard, and it’s the entire purpose of  NDIF. A backend might scale to multiple users by (1) catching a graph request from the frontend, (2) compiling it, (3) sending an intervention graph to nnsight, and (4) passing the webhook to the frontend rather than threading some long running request.</li>
  <li>There isn’t a lot of visual feedback if interventions don’t work.</li>
  <li>It wouldn’t be too difficult to implement training probes or LoRA.</li>
</ul>

<h2>Acknowledgements</h2>
<p>Much thanks to Jaden Fiotto Kaufmann for building the NNsight library.</p>
    </div>

    <script src="/static/js/cord.js"></script>
</body>

</html>